# -*- coding: utf-8 -*-
"""Chatbot_with_memory.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Oy6gHCDwX15O8d5_U-W2Hi9Zo0KH-sH

### Chat PDF with Memory
"""

!pip install chromadb==0.4.6
!pip install pydantic==1.10
!pip install sentence-transformers
!pip install langchain
!pip install huggingface_hub
!pip install transformers
!pip install pypdf

import langchain
import chromadb

import os
import getpass

from langchain.document_loaders import PyPDFLoader  #document loader: https://python.langchain.com/docs/modules/data_connection/document_loaders
from langchain.text_splitter import RecursiveCharacterTextSplitter  #document transformer: text splitter for chunking
from langchain.embeddings import HuggingFaceEmbeddings
from langchain import PromptTemplate
from langchain.vectorstores import Chroma #vector store
from langchain import HuggingFaceHub  #model hub
from langchain.chains import RetrievalQA

from langchain.memory import ConversationBufferMemory

#loading the API key
import getpass
import os
os.environ['HUGGING_FACE_HUB_API_KEY'] = getpass.getpass('Hugging face api key:')

path = "MrBeast.pdf" #input("Enter PDF file path: ")#"C:/Users/Sourav/Downloads/Na√Øve Bayes.pdf"
loader = PyPDFLoader(path)
pages = loader.load()

#number of pages
len(pages)

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)
docs = splitter.split_documents(pages)

embeddings = HuggingFaceEmbeddings()
doc_search = Chroma.from_documents(docs, embeddings)

repo_id = 'lmsys/fastchat-t5-3b-v1.0'  # has 3B parameters: https://huggingface.co/lmsys/fastchat-t5-3b-v1.0
llm = HuggingFaceHub(huggingfacehub_api_token=os.environ['HUGGING_FACE_HUB_API_KEY'],
                     repo_id=repo_id,
                     model_kwargs={'temperature':1e-10, 'max_length':32})

template = """
Use the following context (delimited by <ctx></ctx>) and the chat history (delimited by <hs></hs>) to answer the question:
------
<ctx>
{context}
</ctx>
------
<hs>
{history}
</hs>
------
{question}
Answer:
"""
prompt = PromptTemplate(
    input_variables=["history", "context", "question"],
    template=template,
)

memory = ConversationBufferMemory(
    memory_key="history",
    input_key="question"
)

retrieval_chain = RetrievalQA.from_chain_type(llm,
                                              chain_type='stuff',
                                              retriever=doc_search.as_retriever(),
                                              chain_type_kwargs={
                                                  "prompt":prompt,
                                                  "memory": memory
                                              })

query = "How many subscribers does Mr beast have?"
retrieval_chain.run(query)

query = "What do you remember"
retrieval_chain.run(query)

memory.load_memory_variables({})